{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/user/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "| id|            Country|Year|TotalDemand|TotalProduction|HydroProduction|NuclearProduction|SolarProduction|WindProduction|\n",
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "|  2|        Afghanistan|2015|     4752.6|         1033.7|          889.5|              0.0|            0.0|           0.0|\n",
      "| 28|            Albania|2015|     7265.0|         5895.0|         5895.0|              0.0|            0.0|           0.0|\n",
      "| 55|            Algeria|2015|    62181.0|        68798.0|          145.0|              0.0|           58.0|          19.0|\n",
      "| 81|     American Samoa|2015|      169.4|          169.4|            0.0|              0.0|            1.5|           0.0|\n",
      "|107|            Andorra|2015|    551.381|           99.4|           85.6|              0.0|            0.0|           0.0|\n",
      "|133|             Angola|2015|     9523.0|         9764.0|         5192.0|              0.0|            0.0|           0.0|\n",
      "|159|           Anguilla|2015|       91.5|           94.3|            0.0|              0.0|            0.0|           0.0|\n",
      "|185|Antigua and Barbuda|2015|      337.0|          340.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|211|          Argentina|2015|   149958.0|       145447.0|        38529.0|           7139.0|           15.0|         599.0|\n",
      "|238|            Armenia|2015|     6180.0|         7799.0|         2206.0|           2788.0|            1.0|           3.0|\n",
      "|262|              Aruba|2015|      931.6|          931.6|            0.0|              0.0|            0.0|         138.4|\n",
      "|289|          Australia|2015|   239335.0|       252360.0|        13445.0|              0.0|         5968.0|       11467.0|\n",
      "|316|            Austria|2015|    66539.0|        65299.0|        40592.0|              0.0|          937.0|        4840.0|\n",
      "|342|         Azerbaijan|2015|    23549.0|        24688.0|         1637.0|              0.0|            5.0|           5.0|\n",
      "|366|            Bahamas|2015|     2041.0|         2117.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|393|            Bahrain|2015|    28466.0|        29987.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|419|         Bangladesh|2015|    55696.0|        59011.0|          566.0|              0.0|          154.0|           4.0|\n",
      "|445|           Barbados|2015|      953.6|        1029.29|            0.0|              0.0|            0.0|           0.0|\n",
      "|471|            Belarus|2015|    34604.0|        34077.0|          107.0|              0.0|            3.0|          26.0|\n",
      "|495|            Belgium|2015|    87737.0|        70648.0|         1418.0|          26103.0|         3065.0|        5574.0|\n",
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 224\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Datasets/M2.csv', header=True, inferSchema=True)\n",
    "df = df.filter('Year = 2015').select('id','Country','Year',df.TotalDemand.cast('double').alias('TotalDemand'),df.TotalProduction.cast('double').alias('TotalProduction'),'HydroProduction','NuclearProduction','SolarProduction','WindProduction')\n",
    "df.show()\n",
    "print(\"Total data points:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|            Country|\n",
      "+-------------------+\n",
      "|        Afghanistan|\n",
      "|            Albania|\n",
      "|            Algeria|\n",
      "|     American Samoa|\n",
      "|            Andorra|\n",
      "|             Angola|\n",
      "|           Anguilla|\n",
      "|Antigua and Barbuda|\n",
      "|          Argentina|\n",
      "|            Armenia|\n",
      "|              Aruba|\n",
      "|          Australia|\n",
      "|            Austria|\n",
      "|         Azerbaijan|\n",
      "|            Bahamas|\n",
      "|            Bahrain|\n",
      "|         Bangladesh|\n",
      "|           Barbados|\n",
      "|            Belarus|\n",
      "|            Belgium|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country = df.select('Country')\n",
    "country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "|            Country|Year|TotalDemand|TotalProduction|HydroProduction|NuclearProduction|SolarProduction|WindProduction|\n",
      "+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "|        Afghanistan|2015|     4752.6|         1033.7|          889.5|              0.0|            0.0|           0.0|\n",
      "|            Albania|2015|     7265.0|         5895.0|         5895.0|              0.0|            0.0|           0.0|\n",
      "|            Algeria|2015|    62181.0|        68798.0|          145.0|              0.0|           58.0|          19.0|\n",
      "|     American Samoa|2015|      169.4|          169.4|            0.0|              0.0|            1.5|           0.0|\n",
      "|            Andorra|2015|    551.381|           99.4|           85.6|              0.0|            0.0|           0.0|\n",
      "|             Angola|2015|     9523.0|         9764.0|         5192.0|              0.0|            0.0|           0.0|\n",
      "|           Anguilla|2015|       91.5|           94.3|            0.0|              0.0|            0.0|           0.0|\n",
      "|Antigua and Barbuda|2015|      337.0|          340.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|          Argentina|2015|   149958.0|       145447.0|        38529.0|           7139.0|           15.0|         599.0|\n",
      "|            Armenia|2015|     6180.0|         7799.0|         2206.0|           2788.0|            1.0|           3.0|\n",
      "|              Aruba|2015|      931.6|          931.6|            0.0|              0.0|            0.0|         138.4|\n",
      "|          Australia|2015|   239335.0|       252360.0|        13445.0|              0.0|         5968.0|       11467.0|\n",
      "|            Austria|2015|    66539.0|        65299.0|        40592.0|              0.0|          937.0|        4840.0|\n",
      "|         Azerbaijan|2015|    23549.0|        24688.0|         1637.0|              0.0|            5.0|           5.0|\n",
      "|            Bahamas|2015|     2041.0|         2117.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|            Bahrain|2015|    28466.0|        29987.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|         Bangladesh|2015|    55696.0|        59011.0|          566.0|              0.0|          154.0|           4.0|\n",
      "|           Barbados|2015|      953.6|        1029.29|            0.0|              0.0|            0.0|           0.0|\n",
      "|            Belarus|2015|    34604.0|        34077.0|          107.0|              0.0|            3.0|          26.0|\n",
      "|            Belgium|2015|    87737.0|        70648.0|         1418.0|          26103.0|         3065.0|        5574.0|\n",
      "+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Balance\n",
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "| id|            Country|Year|TotalDemand|TotalProduction|HydroProduction|NuclearProduction|SolarProduction|WindProduction|\n",
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "|  2|        Afghanistan|2015|     4752.6|         1033.7|          889.5|              0.0|            0.0|           0.0|\n",
      "| 28|            Albania|2015|     7265.0|         5895.0|         5895.0|              0.0|            0.0|           0.0|\n",
      "| 55|            Algeria|2015|    62181.0|        68798.0|          145.0|              0.0|           58.0|          19.0|\n",
      "| 81|     American Samoa|2015|      169.4|          169.4|            0.0|              0.0|            1.5|           0.0|\n",
      "|107|            Andorra|2015|    551.381|           99.4|           85.6|              0.0|            0.0|           0.0|\n",
      "|133|             Angola|2015|     9523.0|         9764.0|         5192.0|              0.0|            0.0|           0.0|\n",
      "|159|           Anguilla|2015|       91.5|           94.3|            0.0|              0.0|            0.0|           0.0|\n",
      "|185|Antigua and Barbuda|2015|      337.0|          340.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|211|          Argentina|2015|   149958.0|       145447.0|        38529.0|           7139.0|           15.0|         599.0|\n",
      "|238|            Armenia|2015|     6180.0|         7799.0|         2206.0|           2788.0|            1.0|           3.0|\n",
      "|262|              Aruba|2015|      931.6|          931.6|            0.0|              0.0|            0.0|         138.4|\n",
      "|289|          Australia|2015|   239335.0|       252360.0|        13445.0|              0.0|         5968.0|       11467.0|\n",
      "|316|            Austria|2015|    66539.0|        65299.0|        40592.0|              0.0|          937.0|        4840.0|\n",
      "|342|         Azerbaijan|2015|    23549.0|        24688.0|         1637.0|              0.0|            5.0|           5.0|\n",
      "|366|            Bahamas|2015|     2041.0|         2117.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|393|            Bahrain|2015|    28466.0|        29987.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|419|         Bangladesh|2015|    55696.0|        59011.0|          566.0|              0.0|          154.0|           4.0|\n",
      "|445|           Barbados|2015|      953.6|        1029.29|            0.0|              0.0|            0.0|           0.0|\n",
      "|471|            Belarus|2015|    34604.0|        34077.0|          107.0|              0.0|            3.0|          26.0|\n",
      "|495|            Belgium|2015|    87737.0|        70648.0|         1418.0|          26103.0|         3065.0|        5574.0|\n",
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`Year`' given input columns: [avg(TotalDemand), avg(SolarProduction), avg(HydroProduction), Country, avg(TotalProduction), avg(WindProduction), avg(NuclearProduction), avg(id)];;\\n'Sort ['Year ASC NULLS FIRST], true\\n+- Aggregate [Country#1], [Country#1, avg(cast(id#0 as bigint)) AS avg(id)#115, avg(TotalDemand#22) AS avg(TotalDemand)#116, avg(TotalProduction#23) AS avg(TotalProduction)#117, avg(HydroProduction#6) AS avg(HydroProduction)#118, avg(NuclearProduction#7) AS avg(NuclearProduction)#119, avg(SolarProduction#8) AS avg(SolarProduction)#120, avg(WindProduction#9) AS avg(WindProduction)#121]\\n   +- Project [id#0, Country#1, Year#2, cast(TotalDemand#4 as double) AS TotalDemand#22, cast(TotalProduction#5 as double) AS TotalProduction#23, HydroProduction#6, NuclearProduction#7, SolarProduction#8, WindProduction#9]\\n      +- Filter (cast(Year#2 as double) = cast(2015 as double))\\n         +- Relation[id#0,Country#1,Year#2,IsEstimate#3,TotalDemand#4,TotalProduction#5,HydroProduction#6,NuclearProduction#7,SolarProduction#8,WindProduction#9] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o59.sort.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Year`' given input columns: [avg(TotalDemand), avg(SolarProduction), avg(HydroProduction), Country, avg(TotalProduction), avg(WindProduction), avg(NuclearProduction), avg(id)];;\n'Sort ['Year ASC NULLS FIRST], true\n+- Aggregate [Country#1], [Country#1, avg(cast(id#0 as bigint)) AS avg(id)#115, avg(TotalDemand#22) AS avg(TotalDemand)#116, avg(TotalProduction#23) AS avg(TotalProduction)#117, avg(HydroProduction#6) AS avg(HydroProduction)#118, avg(NuclearProduction#7) AS avg(NuclearProduction)#119, avg(SolarProduction#8) AS avg(SolarProduction)#120, avg(WindProduction#9) AS avg(WindProduction)#121]\n   +- Project [id#0, Country#1, Year#2, cast(TotalDemand#4 as double) AS TotalDemand#22, cast(TotalProduction#5 as double) AS TotalProduction#23, HydroProduction#6, NuclearProduction#7, SolarProduction#8, WindProduction#9]\n      +- Filter (cast(Year#2 as double) = cast(2015 as double))\n         +- Relation[id#0,Country#1,Year#2,IsEstimate#3,TotalDemand#4,TotalProduction#5,HydroProduction#6,NuclearProduction#7,SolarProduction#8,WindProduction#9] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:287)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:280)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:280)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:58)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2850)\n\tat org.apache.spark.sql.Dataset.sortInternal(Dataset.scala:2838)\n\tat org.apache.spark.sql.Dataset.sort(Dataset.scala:1038)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a192dcfdd89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sorted by Balance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, *cols, **kwargs)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \"\"\"\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`Year`' given input columns: [avg(TotalDemand), avg(SolarProduction), avg(HydroProduction), Country, avg(TotalProduction), avg(WindProduction), avg(NuclearProduction), avg(id)];;\\n'Sort ['Year ASC NULLS FIRST], true\\n+- Aggregate [Country#1], [Country#1, avg(cast(id#0 as bigint)) AS avg(id)#115, avg(TotalDemand#22) AS avg(TotalDemand)#116, avg(TotalProduction#23) AS avg(TotalProduction)#117, avg(HydroProduction#6) AS avg(HydroProduction)#118, avg(NuclearProduction#7) AS avg(NuclearProduction)#119, avg(SolarProduction#8) AS avg(SolarProduction)#120, avg(WindProduction#9) AS avg(WindProduction)#121]\\n   +- Project [id#0, Country#1, Year#2, cast(TotalDemand#4 as double) AS TotalDemand#22, cast(TotalProduction#5 as double) AS TotalProduction#23, HydroProduction#6, NuclearProduction#7, SolarProduction#8, WindProduction#9]\\n      +- Filter (cast(Year#2 as double) = cast(2015 as double))\\n         +- Relation[id#0,Country#1,Year#2,IsEstimate#3,TotalDemand#4,TotalProduction#5,HydroProduction#6,NuclearProduction#7,SolarProduction#8,WindProduction#9] csv\\n\""
     ]
    }
   ],
   "source": [
    "print(\"Sorted by Balance\")\n",
    "df.show()\n",
    "df.groupBy('Country').mean().orderBy('Year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('linearRegression').getOrCreate()\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = df.select('TotalProduction',(df.HydroProduction + df.NuclearProduction + df.SolarProduction + df.WindProduction).alias('TotalRenewable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|TotalProduction|TotalRenewable|\n",
      "+---------------+--------------+\n",
      "|         1033.7|         889.5|\n",
      "|         5895.0|        5895.0|\n",
      "|        68798.0|         222.0|\n",
      "|          169.4|           1.5|\n",
      "|           99.4|          85.6|\n",
      "|         9764.0|        5192.0|\n",
      "|           94.3|           0.0|\n",
      "|          340.0|           0.0|\n",
      "|       145447.0|       46282.0|\n",
      "|         7799.0|        4998.0|\n",
      "|          931.6|         138.4|\n",
      "|       252360.0|       30880.0|\n",
      "|        65299.0|       46369.0|\n",
      "|        24688.0|        1647.0|\n",
      "|         2117.0|           0.0|\n",
      "|        29987.0|           0.0|\n",
      "|        59011.0|         724.0|\n",
      "|        1029.29|           0.0|\n",
      "|        34077.0|         136.0|\n",
      "|        70648.0|       36160.0|\n",
      "+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='TotalProduction', labelCol='TotalRenewable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'requirement failed: Column TotalProduction must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o340.fit.\n: java.lang.IllegalArgumentException: requirement failed: Column TotalProduction must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.Predictor.validateAndTransformSchema(Predictor.scala:72)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:122)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-41911dc22ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlrModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Column TotalProduction must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually DoubleType.'"
     ]
    }
   ],
   "source": [
    "lrModel = lr.fit(training)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainingSummary = lrModel.summary\n",
    "\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "trainingSummary.residuals.show()\n",
    "\n",
    "# Print Root Mean Squared Error. \n",
    "print(\"RMSE: {}\".format(trainingSummary.rootMeanSquaredError))\n",
    "\n",
    "# Print R-Squared.\n",
    "print(\"R2: {}\".format(trainingSummary.r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "| id|            Country|Year|TotalDemand|TotalProduction|HydroProduction|NuclearProduction|SolarProduction|WindProduction|\n",
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "|  3|        Afghanistan|2014|     4702.1|         1049.3|          895.3|              0.0|            0.0|           0.0|\n",
      "| 29|            Albania|2014|    7791.43|        4724.43|        4724.43|              0.0|            0.0|           0.0|\n",
      "| 56|            Algeria|2014|    57502.0|        64242.0|          254.0|              0.0|            0.0|           0.0|\n",
      "| 82|     American Samoa|2014|    147.364|        156.945|            0.0|              0.0|            1.1|           0.0|\n",
      "|108|            Andorra|2014|      542.0|          126.8|          113.4|              0.0|            0.0|           0.0|\n",
      "|134|             Angola|2014|     9246.0|         9480.0|         5041.0|              0.0|            0.0|           0.0|\n",
      "|160|           Anguilla|2014|       86.7|           89.5|            0.0|              0.0|            0.0|           0.0|\n",
      "|186|Antigua and Barbuda|2014|      329.0|          336.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|212|          Argentina|2014|   144073.0|       138576.0|        41335.0|           5756.0|           16.0|         619.0|\n",
      "|239|            Armenia|2014|    6281.14|         7750.0|         1992.0|           2465.0|            0.0|           4.0|\n",
      "|263|              Aruba|2014|      916.6|          916.6|            0.0|              0.0|            0.0|         138.4|\n",
      "|290|          Australia|2014|   233408.0|       248299.0|        18421.0|              0.0|         4858.0|       10252.0|\n",
      "|317|            Austria|2014|    66328.0|        65442.0|        44836.0|              0.0|          785.0|        3846.0|\n",
      "|343|         Azerbaijan|2014|    23365.0|        24728.0|         1300.0|              0.0|            3.0|           2.0|\n",
      "|367|            Bahamas|2014|     1848.0|         1922.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|394|            Bahrain|2014|    27253.0|        27253.0|            0.0|              0.0|            0.0|           0.0|\n",
      "|420|         Bangladesh|2014|    52707.0|        55845.0|          588.0|              0.0|          145.0|           4.0|\n",
      "|446|           Barbados|2014|     962.44|        1020.58|            0.0|              0.0|            0.0|           0.0|\n",
      "|472|            Belarus|2014|    35836.0|        34735.0|          121.0|              0.0|            1.0|          11.0|\n",
      "|496|            Belgium|2014|    86004.0|        72672.0|         1462.0|          33703.0|         2883.0|        4615.0|\n",
      "+---+-------------------+----+-----------+---------------+---------------+-----------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total data points: 224\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.csv('Datasets/M2.csv', header=True, inferSchema=True)\n",
    "df1 = df1.filter('Year = 2014').select('id','Country','Year',df1.TotalDemand.cast('double').alias('TotalDemand'),df1.TotalProduction.cast('double').alias('TotalProduction'),'HydroProduction','NuclearProduction','SolarProduction','WindProduction')\n",
    "df1.show()\n",
    "print(\"Total data points:\", df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
